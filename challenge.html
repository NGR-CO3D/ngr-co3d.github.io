---
layout: default
title: " - Common Objects in 3D Challenge"
---

<!-- Main -->
<article id="main">
  <!-- One -->
  <section class="wrapper style3 container">
    <!-- Content -->
    <div class="content">
      <section>
        <header>
          <h2>Common Objects in 3D Challenge @ ECCV'22 NGR-CO3D Workshop</h2>
        </header>
        <p>
          Based on the <a href="https://github.com/facebookresearch/co3d" target="_blank">CO3Dv2 dataset</a>, the challenge will encourage the community to develop methods for reconstructing a large variety of objects from many or few views of a scene. CO3D includes common objects from the COCO taxonomy, with a focus on rigid classes such as fire hydrants, potted plants, balls, cups, etc. The dataset consists of 40k turntable-like videos of such objects, crowd-sourced by nonexperts using cellphone cameras.
        </p>
        <p>
          The challenge comprises two tasks:
        </p>
        <ul>
          <li>
            <h4 style="text-transform: none; font-weight: normal;">
              Task 1 - Many-View Reconstruction:
            </h4>
            <p>
              The goal is to generate the unknown frames in the training videos. Specifically, one has access to a large number of views of an object and the goal is to generate the missing ones — a setup popularised by methods such as NeRF that “overfit” a model to individual videos. This task is representative of applications where one deliberately captures data to reconstruct an object, as in photogrammetry for asset creation for Computer Graphics / Augmented or Virtual Reality.
            </p>
          </li>
          <li>
            <h4 style="text-transform: none; font-weight: normal;">
              Task 2 - Few-View Reconstruction:
            </h4>
            <p>
              The goal is to generate the unknown frames in the testing videos. The task is similar to Task 1, except that only a very small number of views (1-9) from the testing videos with known category labels are available. Reconstruction and novel-view synthesis are likely only possible by learning suitable object priors from the training video collection to fill the “missing gaps”: the object details that cannot be inferred by solely analysing the few images available at test time. This task is representative of applications where one wishes to reconstruct an object captured from casually-recorded data, such as in most egocentric videos.
            </p>
          </li>
        </ul>
        <p>
          We will be releasing a new version of the dataset CO3Dv2, which consists of 36.5k videos of 50 object categories from the MS-COCO taxonomy (with nearly doubled amount of data and improved annotations compared to the first version). The test set of the challenge contains 20k videos where, for fairness, only the “known” frames will be publicly available. We have manually checked every test video and its 3D annotations to ensure reliable ground truth for evaluation.
        </p>

        <h3 style="text-transform: none; font-weight: normal;">
          Submission Guidelines:
        </h3>
        <p>
          The challenge is hosted on <a href="https://eval.ai/web/challenges/challenge-page/1819/overview" target="_blank">EvalAI</a>. Please check the submission guidelines <a href="https://github.com/facebookresearch/co3d/blob/main/co3d/challenge/README.md" target="_blank">here</a>.
        </p>

        <h3 style="text-transform: none; font-weight: normal;">
          Timeline:
        </h3>
        <table class="alt">
          <tbody>
            <col width="20%">
            <col width="50%">
            <col width="30%">
            <tr>
              <td>late Jul</td>
              <td>Submission portal open</td>
            </tr>
            <tr>
              <td>mid Oct</td>
              <td>Submission deadline</td>
            </tr>
            <tr>
              <td>Oct 24</td>
              <td>Day of workshop @ECCV'22</td>
            </tr>
          </tbody>
        </table>

        <h3 style="text-transform: none; font-weight: normal;">
          Questions?
        </h3>
        <p>
          If you have any questions, feel free to <a href="mailto:dnovotny@fb.com,szwu@robots.ox.ac.uk,romansh@fb.com,sinhasam@fb.com">contact the organizers</a>: David Novotny (dnovotny@fb.com), Shangzhe Wu (szwu@robots.ox.ac.uk), Roman Shapovalov (romansh@fb.com), Samarth Sinha (sinhasam@fb.com).
        </p>
        
      </section>
    </div>
  </section>

</article>
